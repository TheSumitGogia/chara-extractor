django (specifically django.utils.encoding)
BeautifulSoup
networkx
fuzzywuzzy
NumPy/SciPy
matplotlib
stop_words
nltk
scikit-learn
gutenberg
Stanford CoreNLP

Our Scripts:

annotation_check
    verify whether "matching" for annotations to candidates captures SN data
bname_extractor
    sample books (for evaluation of Sparknotes labeler)
crossref
    check for plays/poetry/language with google to verify gutenberg books
data_collect
    extracts book names and character descriptions from Sparknotes
disambiguation
    takes candidates and links them if similar "name-wise" (heuristic)
evaluation
    takes characters, (char clf output), matches and finds P/R
extraction (sh)
    runs feature parser for different parameter sets given nlp, tokens dirs
get_text
    takes good book list and (fixed ugly) book list, gets raw text from PG
labeling
    takes books, features (candidate list), and assigns labels (annotations)
parsing
    runs Stanford CoreNLP on sparknotes, books
process_sparknote
    finds characters and relations from sparknotes descriptions
split_files
    partitions corenlp, tokens, text dirs (distribute feature extraction)
train_char
    run different training algorithms on feature matrices for char data
train_pair
    run different training algorithms on feature matrices for pair data
train_common
    contains util functions for training
        translate feature files into matrices
        translate labels into matrices
        split training and test data
training
    ?? deprecated??
    has bunch of util functions and training functions
wordnet_hyponyms
    for getting hyponyms of words, used for "person" hyponyms

Spec (use for merging files, re-organizing code):
    Labeling
        Get Sparknotes Data
        (Get Books -> Call Gutenberg)
        (Parse Books -> Call Data Parser)
        (Get Candidates -> Call Data Parser)
        (Get Candidates -> Call Feature Extractor)
        Apply Sparknotes Labels
        Check Sparknotes Labels 
        Sample Books (for labeling Sparknotes labeler test data)
            - note: should leave default (we already made tags)
    Gutenberg
        (Need List of Viable Books) 
        CrossRef
        Get Books 
    Data Parser
        Run CoreNLP
            Sparknotes (token, split, pos, parse, ner, coref)
            Raw texts (tokenize, ssplit, pos, ner)
            Raw texts (special tokenization) 
        Section
        Get Candidates 
    Feature Extractor
        Get Count Features
        Get Tag Features
        Get Coref Features
        Get Cooc Features 
    Learning
        Translate Features
        Train/Test Split 
        Train w/ Algorithm
            Chars
            Pairs
    Evaluation
        Check Classifier p/r
        Check Classifier disambig p/r
        Test Book 
        Test Books
    Disambiguation 
    Pipeline
        (Allows user to run parts of pipeline for project)
        Labeling -> Feature Extraction -> Learning -> Evaluation 
