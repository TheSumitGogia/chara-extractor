\documentclass[12pt]{article}

% Any percent sign marks a comment to the end of the line

% Every latex document starts with a documentclass declaration like this
% The option dvips allows for graphics, 12pt is the font size, and article
%   is the style

\usepackage{amsmath, amssymb}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{float}
\usepackage[margin=1.0in]{geometry}
\usepackage[font={small,it}]{caption}

% These are additional packages for "pdflatex", graphics, and to include
% hyperlinks inside a document.

%\setlength{\oddsidemargin}{0.25in}
%\setlength{\textwidth}{6.5in}
%\setlength{\topmargin}{0in}
%\setlength{\textheight}{8.5in}
\setlength{\parindent}{0pt}

% These force using more of the margins that is the default style

\begin{document}


\title{Learning Character Graphs from Literature}
\author{Sumit Gogia, Min Zhang, Tommy Zhang}
\date{\today}

\maketitle

\begin{abstract}

We present a method for extracting salient characters and recognizing salient character relationships from literature. As opposed to previous work which extracts \emph{social} networks by examining dialogue \emph{given} characters, our trained system finds characters and relationships directly from raw novel text. Our approach utilizes a novel supervised learning approach where we retrieve labels from a simple, digestible online source (Sparknotes) to train classifiers that identify characters as salient or pairs of characters as related. Initial results show that even basic classification methods produce good performance for salient character extraction, while relationship detection warrants significant improvement. 

\end{abstract}

\section{Introduction}

Literary scholars, when comparing different works of literature, frequently examine major characters and their relationships for comparison. These examinations utilize attributes such as number of major characters, names of characters, and types of relationships in each novel to make higher-level statements about novel form. \\

Unfortunately, at current, the approaches to such examination are typically manual and require close reading, preventing comparison of the large existing bodies of literature. While statements can and have been made for a small manageable subset, this notably results in a heavily-studied canon and a largely-ignored general body of literature (insert Moretti reference). A conclusion literary scholars have come to is this: we need efficient \emph{automated} methods for extracting higher-level representations from literature (also Moretti reference). \\

Recent years have seen work founded in this realization, both for character analysis as well as other high-level analyses such as plot extraction. In character analysis, methods have focused on \emph{social network extraction}, as many literary theories involve the communication between characters (insert references). These methods take lists of characters in a novel and then describe their relationships in a graph, with edges connecting characters indicating relationships. They utilize \emph{dialogue} as the basis for their analysis, extracting dialogue from raw text and attributing it to characters to compute edge weights and node sizes. \\

However, despite the success of these methods, they do not address many of the needs for automated character examination, including automated extraction of many attributes noted above. The goal in this paper is to supplement automated social network extraction with automated methods for extracting two useful representations:  

\begin{enumerate}
    \setlength\itemsep{0em}
    \item A list of major characters in a novel. 
    \item A list of pairs of major characters with a salient relationship in a novel. 
\end{enumerate}

Importantly, these representations are different from those found in social network extraction. Firstly, social network extractors have character lists as input instead of output - our character extractor can feed output into social network extractors. Secondly, social network relationships are found through dialogue only as opposed to the text entirety. A graphic describing the representations found by social network extractors and our extractors is given below for clarity. 

\begin{figure}[H]
    \centering
    %\includegraphics[width=6in]{rep_graphs.png}
    \caption{Representations output by social network extractors (left) and our extractors (right). Note that social network extractors take nodes as input and output weighted or labeled edges, while ours outputs nodes and binary edges. Our extractors also utilize text entirety, as opposed to dialogue alone.} 
\end{figure}

Our approach to creating these extractors is to use binary classifiers that act on broad sets of automatically-found character candidates and pairs of major characters, to detect salient characters and relationships respectively. We develop a set of global novel-wide features, such as candidate count and coocurrence, and a novel automated labeling method where we read Sparknotes (insert reference), an online reference with character lists and descriptions for literature, and match their data to ours. 

% NOTE: Might want to reword previous paragraph to summarize contributions
% NOTE: Might want to add brief paper sectioning overview

\section{Related Work}
% GENERAL OVERVIEW, VERY BRIEF

\subsection{Named Entity Recognition}

    Our first problem of character extraction can is closely related to NER problems. The goal in NER tasks
    is to label text token sequences as special entities with respect to the rest of the text. Similarly, in 
    character extraction the goal is to recognize identifiable unique references to major characters, which
    we take to fall in the class of consecutive token sequences. \\

    As such, we can look to previous work on NER problems to influence our design. A large portion of NER systems
    attempt to label special entities in short, stand-alone documents based on local features for token
    sequences, such as capitalization, prefixes and suffixes (insert references here). These types of word-level
    features are important in our case as well, but given the basic hypothesis that the salience of characters 
    is dependent on how they appear throughout the book, we can imagine that they would not prove particularly 
    effective. \\

    On the other hand, NER systems which target longer types or sets of documents or attempt to focus on
    salient entities also exist, and use additional features to significantly boost performance. 
    Previous work in this vein includes systems which target extraction of important figures 
    from news articles (insert reference) and identification of proper nouns in Chinese text (insert reference).
    The systems described utilize features such as candidate entity frequency and presence of other
    entities in vicinity to help capture the comparative importance of desirable entities. Given that in 
    character extraction we aim to identify \emph{salient} characters, these works significantly influence
    our own. \\

\subsection{Social Network Extraction}

    Previous character-based attribute extraction work has focused on social network extraction.
    Social network extraction consists of tasks similar to the relationship labeling task we focus on, but which 
    focus on a representation influenced only by dialogue in novels. The tasks in social network
    extraction tend to be considerably more well-defined than those we explore - in (insert reference),
    the authors identify the amount of dialogue between characters to represent the network, and in
    (insert reference) the authors identify other determinstic features for dialogue such as
    point-of-view in narration. While the language processing can be difficult to determine these
    features, they are fairly clear to human annotators. \\

    In contrast, in both our character extraction and relationship identification tasks, we identify
    \emph{salient} characters and relationships. This concept of \emph{salience} is ambiguous even
    to many literary scholars; it is often difficult to agree on which characters and relationships
    are important in novels due to novel complexity, and context which each reader brings with them.
    We note that ideally, the tasks we wish to address would be satisfied by tools that adapt to
    different users' perspectives, possibly taking into account personal annotations. In this work,
    we attempt to overcome this problem by using a source which has aggregated many perspectives,
    Sparknotes (insert reference), but it is worth noting that given the novel language isn't
    enough to fully define \emph{salience} for humans, there is extra complexity in the language
    processing task. \\

    Despite this difference, it is not to say that the approaches to social network extraction are
    unrelated to our work. The features used in models for dialogue detection and dialogue attribute
    detection, such as number of speakers, can potentially be useful in our tasks as well. We make use
    of some, like cooccurrence features, but there are many that we have not that could be tested
    in future work.

\section{Methodology}
\begin{figure}[H]
    \centering
    %\includegraphics[width=7in]{pipeline.png}
    \caption{The training and extraction pipelines for both our character extractor and relationship identifier.
        Note that in relationship identification, the training data consists of pairs of candidates labeled as
    characters given Sparknotes.}
\end{figure}

Our approach is to treat both character extraction and relationship identification as binary classification problems. 
We thus have two main stages for our system: a training stage and an execution stage. 
In the training stage, we collect data, extract features, apply labels, and then train classifiers; 
in the execution stage we run our extractors on raw test novels, extracting unlabeled data with features 
and running the classifiers on it. Pipelines for both of these stages are shown in the figure above. 
In the following sections, we discuss the components of both pipelines in further detail.\\

    \subsection{Candidate Extraction}
        Our classifiers, rather than run on all token sequences in text, are both trained and executed on
        broad sets of multi-token character \emph{candidates} obtained from the raw text automatically. 
        As in certain multi-token NER systems (J Da Silva), this strategy is employed because we do not
        know a priori the number of tokens for characters and want to avoid training on large numbers
        of clearly negative examples. In addition, it is also because the complexity of our feature extraction 
        depends on the number of candidates through co-occurrence counting. \\

        The method for candidate extraction is very tied to our understanding of character names: we assume
        that character names accord to the following rules:
        \begin{enumerate}
            \item A name is a noun phrases with at most one level of nesting and ending in a capitalized noun. OR
            \item A name is a noun phrase consisting of a determiner and a hyponyms ``person" in WordNet (reference). 
        \end{enumerate}
        A diagram describing these rules and examples is shown below for clarity.

        \begin{figure}[H]
            \centering
            %\includegraphics{}
            \caption{A depiction of the rules for candidate extraction.}
        \end{figure}

        We extract all token sequences satisfying these rules as candidates.
        While many character names are captured just by taking consecutive capitalized tokens, the rules
        are more flexible to account for character names such as \emph{the Count of Monte Cristo} and 
        \emph{the helmsman}, characters in the eponymously-named book and \emph{Heart of Darkness} respectively.
        In the first we have parts of speech other than nouns, while in the second there is no capitalization,
        just the recognition that the word refers to a person. There may be characters names which break
        this rule, but we did not observe any.\\

        This rule-based approach also faces potential issues in that unmeaningful token sequences such as
        ``the woman" in a book with many women may get passed and affect co-occurrence features; however,
        we observe that co-occurrence with even these ``non-characters" is often important, and also
        include weighted co-occurrence features to help account for this issue. Though we do not
        report numerical results, we observe that not including these candidates does not affect 
        end performance.

    \subsection{Feature Extraction}

        We extract four main types of features for both candidates and pairs: tag features,
        coreference features, frequency features, and coocurrence features. The first three are common
        for NER systems, and are included due to the tasks' similarity to NER noted previously. The last
        feature type, also seen in \emph{Related Entity Finding} tasks (insert references), is to account
        for the idea that characters are considered salient due to their relationships with other
        entities in the novel, and that cooccurrence of characters in some section indicates interaction. \\

        There are actually a large number of features included for each feature type, arising from a number
        of normalization schemes we use to account for differences between novels. We do not expect
        all of these to be particularly helpful, and instead count on our learning methods ruling out
        noisy or uncorrelated features. Details for features of each feature type are described below.

        \subsubsection{Tag Features}
        
        What we call \emph{tag} features refer to just features dependent on capitalization of tokens in
        candidates, and whether Stanford's CoreNLP NER tagger (insert reference) labeled tokens as 
        named entities or not. We noted that while current NER systems such as Stanford's were
        not very good at understanding when titles or modifiers were parts of names, they frequently
        recognized given names of characters, like ``Sally". Since character names often 
        include given names, we decided to use the presence of NER tags for indicator features. \\

        As we need to normalize the tag features we obtained over candidates, sequences of tokens,
        we use presence of capitalization or NER ``person" tag on last token, as well as fractions
        of tokens with capitalization or ``person" tags. The NER tagger is non-deterministic so
        we use a fractional count of ``person" tags over all instances of the candidate for NER
        tag features. Tag features for pairs of candidates are just concatenations of the tag
        features for both candidates.

        \subsubsection{Frequency Features}

        Frequency features are indications of how often a candidate appears in the given novel. We include 
        these since we expect salient characters and relationships to be tied to how often they occur. 
        The simplest feature of this type is just the raw frequency of how many times a candidate
        appears in the novel. However, we include many variations on this feature to account for
        differences in book lengths and differences in general candidate reference frequency across books.
        These include frequencies for different section types (sentence, paragraph, and chapter), as well
        as normalization across the number of sections and total number of section mentiones for candidates.
        Explicitly, for section type $s$, we have:
        
        \[\mbox{count}_s(\mbox{candidate}) = \mbox{(\# of sections candidate appears in)}\]
        \[\overline{\mbox{count}}_s(\mbox{candidate}) = \frac{\mbox{count}_s(\mbox{candidate})}{\mbox{\# of sections}}\]
        \[\hat{\mbox{count}}_s(\mbox{candidate}) = \frac{\mbox{count}_s(\mbox{candidate})}{\sum_{\mbox{cand}\in{C}}\mbox{count}_s(\mbox{cand})}\]

        The count over the entire book we normalize by the number of characters (text) in the book, and the total
        counts of all candidates. Again, for pairs of candidates we concatenate the frequency features for both
        candidates.

        \subsubsection{Cooccurrence Features}

        Cooccurrence features describe the number of times candidates appear with each other candidates. 
        While it seems clear that such features would be important for identifying salient relationships,
        that it would be helpful for salient character extraction is less obvious. Our reasoning is that 
        characters in novels are often important precisely because they interact with other
        characters. \\

        The particular features of this type we compute are similar to our frequency features. We identify
        which sections characters occur in to build occurrence matrices, and then compute inner products
        of these occurrence matrices to obtain coocurrence matrices. The features for candidates are
        given by sums of coocurrences with all other candidates. 

        \[\mbox{cooc}_s(\mbox{candidate}) = \sum_{\mbox{c'} \in C} \mbox{cooc}_s(\mbox{candidate}, \mbox{c'})\]

        To normalize across books, as well as try and account for importance of characters, we also
        weight the cooccurrence matrices along $1$ dimension by normalized overall and section frequency
        features. We do not weight across the second dimension (the one not summed) because we desire to
        get the importance of candidates a candidate cooccurs with, not accounting for the importance
        of the original candidate itself.

        % Might want an equation here to describe normalization

        For pairs of characters, we both concatenate the coocurrence features for each character, as well
        as include the values from the original un-marginalized coocurrence matrices.

        \subsubsection{Coreference Features}
        
        Coreference features are to account for cases where characters are referred to by different 
        names within a book. For example, ``Huckleberry Finn" may be addressed as ``Huck", ``Huck Finn",
        or ``Huckleberry", but all are instances of the same character. As we are using different
        types of frequencies (including cooccurrence) to help account for importance, to help
        with measuring a candidate's true importance it should have features recognizing the 
        frequencies of coreferences. \\

        To ensure this, we developed a rule-based disambiguation scheme that accounts for titles,
        nicknames by means of fuzzy matching, and partial reference. The disambiguator takes
        candidate sets and maps each to other candidates which are coreferences. Each candidate then has
        features accounting for the presence of shorter and longer coreferences, as well as
        the total frequencies for those coreferences respectively. While the disambiguation
        scheme is certainly not perfect, we observed it to work well, and relied on the classification
        methods to deal with the noise in it.

    \subsection{Data Collection}
    \subsection{Classification}

\section{Experiments}
    \subsection{Character Extraction}
    \subsection{Relationship Identification}

\section{Conclusion}

\end{document}
